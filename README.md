# PDE-Dx: A Diagnostic Benchmark for LLMs on PDE Reasoning

**PDE-Dx** is an open-source benchmark designed to rigorously test large language models (LLMs) on partial differential equation (PDE)-related tasks. Inspired by diagnostic datasets in scientific machine learning, this benchmark focuses not on abstract difficulty, but on probing whether LLMs truly understand the mathematical structure, solver planning, and physical context behind PDEs.

Rather than evaluating models only on code correctness or surface-level syntax, PDE-Dx introduces challenging yet practical tasks that expose how LLMs reason about:

- Solver selection  
- Boundary condition implications  
- Numerical stability  
- Symbolic simplifications  
- Real-world modeling decisions  

Our goal is to develop a **diagnostic toolkit**â€”a set of benchmarks that help us understand not just whether LLMs can generate working solvers, but how they think through the problem, revealing both strengths and failure modes in reasoning.

ðŸ“Œ **Contributors are invited to co-author the final PDE-Dx paper.** We welcome submissions of challenging tasks and use cases.

ðŸ‘‰ [Submit a task here](https://fazliani-pde-dx.hf.space/) or reach out via [email](mailto:fazliani@stanford.edu).
